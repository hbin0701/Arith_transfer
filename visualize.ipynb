{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# from google.colab import drive\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from functools import *\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# import comet_ml\n",
    "import itertools\n",
    "\n",
    "from model import Transformer\n",
    "\n",
    "# Helper functions\n",
    "def cuda_memory():\n",
    "    print(torch.cuda.memory_allocated()/1e9)\n",
    "\n",
    "def cross_entropy_high_precision(logits, labels):\n",
    "    # Shapes: batch x vocab, batch\n",
    "    # Cast logits to float64 because log_softmax has a float32 underflow on overly\n",
    "    # confident data and can only return multiples of 1.2e-7 (the smallest float x\n",
    "    # such that 1+x is different from 1 in float32). This leads to loss spikes\n",
    "    # and dodgy gradients\n",
    "    logprobs = F.log_softmax(logits.to(torch.float64), dim=-1)\n",
    "    prediction_logprobs = torch.gather(logprobs, index=labels[:, None], dim=-1)\n",
    "    loss = -torch.mean(prediction_logprobs)\n",
    "    return loss\n",
    "\n",
    "def full_loss(model, data):\n",
    "    # Take the final position only\n",
    "    logits = model(data)[:, -1]\n",
    "    labels = torch.tensor([fn(i, j) for i, j, _ in data]).to('cuda')\n",
    "    return cross_entropy_high_precision(logits, labels)\n",
    "\n",
    "def test_logits(logits, bias_correction=False, original_logits=None, mode='all'):\n",
    "    # Calculates cross entropy loss of logits representing a batch of all p^2\n",
    "    # possible inputs\n",
    "    # Batch dimension is assumed to be first\n",
    "    if logits.shape[1]==p*p:\n",
    "        logits = logits.T\n",
    "    if logits.shape==torch.Size([p*p, p+1]):\n",
    "        logits = logits[:, :-1]\n",
    "    logits = logits.reshape(p*p, p)\n",
    "    if bias_correction:\n",
    "        # Applies bias correction - we correct for any missing bias terms,\n",
    "        # independent of the input, by centering the new logits along the batch\n",
    "        # dimension, and then adding the average original logits across all inputs\n",
    "        logits = einops.reduce(original_logits - logits, 'batch ... -> ...', 'mean') + logits\n",
    "    if mode=='train':\n",
    "        return cross_entropy_high_precision(logits[is_train], labels[is_train])\n",
    "    elif mode=='test':\n",
    "        return cross_entropy_high_precision(logits[is_test], labels[is_test])\n",
    "    elif mode=='all':\n",
    "        return cross_entropy_high_precision(logits, labels)\n",
    "    \n",
    "#Plotting functions\n",
    "# This is mostly a bunch of over-engineered mess to hack Plotly into producing\n",
    "# the pretty pictures I want, I recommend not reading too closely unless you\n",
    "# want Plotly hacking practice\n",
    "def to_numpy(tensor, flat=False):\n",
    "    if type(tensor)!=torch.Tensor:\n",
    "        return tensor\n",
    "    if flat:\n",
    "        return tensor.flatten().detach().cpu().numpy()\n",
    "    else:\n",
    "        return tensor.detach().cpu().numpy()\n",
    "def imshow(tensor, xaxis=None, yaxis=None, animation_name='Snapshot', **kwargs):\n",
    "    if tensor.shape[0]==p*p:\n",
    "        tensor = unflatten_first(tensor)\n",
    "    tensor = torch.squeeze(tensor)\n",
    "    px.imshow(to_numpy(tensor, flat=False),\n",
    "              labels={'x':xaxis, 'y':yaxis, 'animation_name':animation_name},\n",
    "              **kwargs).show()\n",
    "# Set default colour scheme\n",
    "imshow = partial(imshow, color_continuous_scale='Blues')\n",
    "# Creates good defaults for showing divergent colour scales (ie with both\n",
    "# positive and negative values, where 0 is white)\n",
    "imshow_div = partial(imshow, color_continuous_scale='RdBu', color_continuous_midpoint=0.0)\n",
    "# Presets a bunch of defaults to imshow to make it suitable for showing heatmaps\n",
    "# of activations with x axis being input 1 and y axis being input 2.\n",
    "inputs_heatmap = partial(imshow, xaxis='Input 1', yaxis='Input 2', color_continuous_scale='RdBu', color_continuous_midpoint=0.0)\n",
    "def line(x, y=None, hover=None, xaxis='', yaxis='', **kwargs):\n",
    "    if type(y)==torch.Tensor:\n",
    "        y = to_numpy(y, flat=True)\n",
    "    if type(x)==torch.Tensor:\n",
    "        x=to_numpy(x, flat=True)\n",
    "    fig = px.line(x, y=y, hover_name=hover, **kwargs)\n",
    "    fig.update_layout(xaxis_title=xaxis, yaxis_title=yaxis)\n",
    "    fig.show()\n",
    "def scatter(x, y, **kwargs):\n",
    "    px.scatter(x=to_numpy(x, flat=True), y=to_numpy(y, flat=True), **kwargs).show()\n",
    "def lines(lines_list, x=None, mode='lines', labels=None, xaxis='', yaxis='', title = '', log_y=False, hover=None, **kwargs):\n",
    "    # Helper function to plot multiple lines\n",
    "    if type(lines_list)==torch.Tensor:\n",
    "        lines_list = [lines_list[i] for i in range(lines_list.shape[0])]\n",
    "    if x is None:\n",
    "        x=np.arange(len(lines_list[0]))\n",
    "    fig = go.Figure(layout={'title':title})\n",
    "    fig.update_xaxes(title=xaxis)\n",
    "    fig.update_yaxes(title=yaxis)\n",
    "    for c, line in enumerate(lines_list):\n",
    "        if type(line)==torch.Tensor:\n",
    "            line = to_numpy(line)\n",
    "        if labels is not None:\n",
    "            label = labels[c]\n",
    "        else:\n",
    "            label = c\n",
    "        fig.add_trace(go.Scatter(x=x, y=line, mode=mode, name=label, hovertext=hover, **kwargs))\n",
    "    if log_y:\n",
    "        fig.update_layout(yaxis_type=\"log\")\n",
    "    fig.show()\n",
    "def line_marker(x, **kwargs):\n",
    "    lines([x], mode='lines+markers', **kwargs)\n",
    "def animate_lines(lines_list, snapshot_index = None, snapshot='snapshot', hover=None, xaxis='x', yaxis='y', **kwargs):\n",
    "    if type(lines_list)==list:\n",
    "        lines_list = torch.stack(lines_list, axis=0)\n",
    "    lines_list = to_numpy(lines_list, flat=False)\n",
    "    if snapshot_index is None:\n",
    "        snapshot_index = np.arange(lines_list.shape[0])\n",
    "    if hover is not None:\n",
    "        hover = [i for j in range(len(snapshot_index)) for i in hover]\n",
    "    print(lines_list.shape)\n",
    "    rows=[]\n",
    "    for i in range(lines_list.shape[0]):\n",
    "        for j in range(lines_list.shape[1]):\n",
    "            rows.append([lines_list[i][j], snapshot_index[i], j])\n",
    "    df = pd.DataFrame(rows, columns=[yaxis, snapshot, xaxis])\n",
    "    px.line(df, x=xaxis, y=yaxis, animation_frame=snapshot, range_y=[lines_list.min(), lines_list.max()], hover_name=hover,**kwargs).show()\n",
    "\n",
    "def imshow_fourier(tensor, title='', animation_name='snapshot', facet_labels=[], **kwargs):\n",
    "    # Set nice defaults for plotting functions in the 2D fourier basis\n",
    "    # tensor is assumed to already be in the Fourier Basis\n",
    "    if tensor.shape[0]==p*p:\n",
    "        tensor = unflatten_first(tensor)\n",
    "    tensor = torch.squeeze(tensor)\n",
    "    fig=px.imshow(to_numpy(tensor),\n",
    "            x=fourier_basis_names,\n",
    "            y=fourier_basis_names,\n",
    "            labels={'x':'x Component',\n",
    "                    'y':'y Component',\n",
    "                    'animation_frame':animation_name},\n",
    "            title=title,\n",
    "            color_continuous_midpoint=0.,\n",
    "            color_continuous_scale='RdBu',\n",
    "            **kwargs)\n",
    "    fig.update(data=[{'hovertemplate':\"%{x}x * %{y}y<br>Value:%{z:.4f}\"}])\n",
    "    if facet_labels:\n",
    "        for i, label in enumerate(facet_labels):\n",
    "            fig.layout.annotations[i]['text'] = label\n",
    "    fig.show()\n",
    "\n",
    "def animate_multi_lines(lines_list, y_index=None, snapshot_index = None, snapshot='snapshot', hover=None, swap_y_animate=False, **kwargs):\n",
    "    # Can plot an animation of lines with multiple lines on the plot.\n",
    "    if type(lines_list)==list:\n",
    "        lines_list = torch.stack(lines_list, axis=0)\n",
    "    lines_list = to_numpy(lines_list, flat=False)\n",
    "    if swap_y_animate:\n",
    "        lines_list = lines_list.transpose(1, 0, 2)\n",
    "    if snapshot_index is None:\n",
    "        snapshot_index = np.arange(lines_list.shape[0])\n",
    "    if y_index is None:\n",
    "        y_index = [str(i) for i in range(lines_list.shape[1])]\n",
    "    if hover is not None:\n",
    "        hover = [i for j in range(len(snapshot_index)) for i in hover]\n",
    "    print(lines_list.shape)\n",
    "    rows=[]\n",
    "    for i in range(lines_list.shape[0]):\n",
    "        for j in range(lines_list.shape[2]):\n",
    "            rows.append(list(lines_list[i, :, j])+[snapshot_index[i], j])\n",
    "    df = pd.DataFrame(rows, columns=y_index+[snapshot, 'x'])\n",
    "    px.line(df, x='x', y=y_index, animation_frame=snapshot, range_y=[lines_list.min(), lines_list.max()], hover_name=hover, **kwargs).show()\n",
    "\n",
    "def animate_scatter(lines_list, snapshot_index = None, snapshot='snapshot', hover=None, yaxis='y', xaxis='x', color=None, color_name = 'color', **kwargs):\n",
    "    # Can plot an animated scatter plot\n",
    "    # lines_list has shape snapshot x 2 x line\n",
    "    if type(lines_list)==list:\n",
    "        lines_list = torch.stack(lines_list, axis=0)\n",
    "    lines_list = to_numpy(lines_list, flat=False)\n",
    "    if snapshot_index is None:\n",
    "        snapshot_index = np.arange(lines_list.shape[0])\n",
    "    if hover is not None:\n",
    "        hover = [i for j in range(len(snapshot_index)) for i in hover]\n",
    "    if color is None:\n",
    "        color = np.ones(lines_list.shape[-1])\n",
    "    if type(color)==torch.Tensor:\n",
    "        color = to_numpy(color)\n",
    "    if len(color.shape)==1:\n",
    "        color = einops.repeat(color, 'x -> snapshot x', snapshot=lines_list.shape[0])\n",
    "    print(lines_list.shape)\n",
    "    rows=[]\n",
    "    for i in range(lines_list.shape[0]):\n",
    "        for j in range(lines_list.shape[2]):\n",
    "            rows.append([lines_list[i, 0, j].item(), lines_list[i, 1, j].item(), snapshot_index[i], color[i, j]])\n",
    "    print([lines_list[:, 0].min(), lines_list[:, 0].max()])\n",
    "    print([lines_list[:, 1].min(), lines_list[:, 1].max()])\n",
    "    df = pd.DataFrame(rows, columns=[xaxis, yaxis, snapshot, color_name])\n",
    "    px.scatter(df, x=xaxis, y=yaxis, animation_frame=snapshot, range_x=[lines_list[:, 0].min(), lines_list[:, 0].max()], range_y=[lines_list[:, 1].min(), lines_list[:, 1].max()], hover_name=hover, color=color_name, **kwargs).show()\n",
    "    \n",
    "def unflatten_first(tensor):\n",
    "    if tensor.shape[0]==p*p:\n",
    "        return einops.rearrange(tensor, '(x y) ... -> x y ...', x=p, y=p)\n",
    "    else:\n",
    "        return tensor\n",
    "def cos(x, y):\n",
    "    return (x.dot(y))/x.norm()/y.norm()\n",
    "def mod_div(a, b):\n",
    "    return (a*pow(b, p-2, p))%p\n",
    "def normalize(tensor, axis=0):\n",
    "    return tensor/(tensor).pow(2).sum(keepdim=True, axis=axis).sqrt()\n",
    "def extract_freq_2d(tensor, freq):\n",
    "    # Takes in a pxpx... or batch x ... tensor, returns a 3x3x... tensor of the\n",
    "    # Linear and quadratic terms of frequency freq\n",
    "    tensor = unflatten_first(tensor)\n",
    "    # Extracts the linear and quadratic terms corresponding to frequency freq\n",
    "    index_1d = [0, 2*freq-1, 2*freq]\n",
    "    # Some dumb manipulation to use fancy array indexing rules\n",
    "    # Gets the rows and columns in index_1d\n",
    "    return tensor[[[i]*3 for i in index_1d], [index_1d]*3]\n",
    "def get_cov(tensor, norm=True):\n",
    "    # Calculate covariance matrix\n",
    "    if norm:\n",
    "        tensor = normalize(tensor, axis=1)\n",
    "    return tensor @ tensor.T\n",
    "def is_close(a, b):\n",
    "    return ((a-b).pow(2).sum()/(a.pow(2).sum().sqrt())/(b.pow(2).sum().sqrt())).item()\n",
    "\n",
    "def fft1d(tensor):\n",
    "    # Converts a tensor with dimension p into the Fourier basis\n",
    "    return tensor @ fourier_basis.T\n",
    "\n",
    "def fourier_2d_basis_term(x_index, y_index):\n",
    "    # Returns the 2D Fourier basis term corresponding to the outer product of\n",
    "    # the x_index th component in the x direction and y_index th component in the\n",
    "    # y direction\n",
    "    # Returns a 1D vector of length p^2\n",
    "    return (fourier_basis[x_index][:, None] * fourier_basis[y_index][None, :]).flatten()\n",
    "\n",
    "def fft2d(mat):\n",
    "    # Converts a pxpx... or batch x ... tensor into the 2D Fourier basis.\n",
    "    # Output has the same shape as the original\n",
    "    shape = mat.shape\n",
    "    mat = einops.rearrange(mat, '(x y) ... -> x y (...)', x=p, y=p)\n",
    "    fourier_mat = torch.einsum('xyz,fx,Fy->fFz', mat, fourier_basis, fourier_basis)\n",
    "    return fourier_mat.reshape(shape)\n",
    "\n",
    "def analyse_fourier_2d(tensor, top_k=10):\n",
    "    # Processes a (p,p) or (p*p) tensor in the 2D Fourier Basis, showing the\n",
    "    # top_k terms and how large a fraction of the variance they explain\n",
    "    values, indices = tensor.flatten().pow(2).sort(descending=True)\n",
    "    rows = []\n",
    "    total = values.sum().item()\n",
    "    for i in range(top_k):\n",
    "        rows.append([tensor.flatten()[indices[i]].item(),\n",
    "                     values[i].item()/total,\n",
    "                     values[:i+1].sum().item()/total,\n",
    "                     fourier_basis_names[indices[i].item()//p],\n",
    "                     fourier_basis_names[indices[i]%p]])\n",
    "    display(pd.DataFrame(rows, columns=['Coefficient', 'Frac explained', 'Cumulative frac explained', 'x', 'y']))\n",
    "\n",
    "def get_2d_fourier_component(tensor, x, y):\n",
    "    # Takes in a batch x ... tensor and projects it onto the 2D Fourier Component\n",
    "    # (x, y)\n",
    "    vec = fourier_2d_basis_term(x, y).flatten()\n",
    "    return vec[:, None] @ (vec[None, :] @ tensor)\n",
    "\n",
    "def get_component_cos_xpy(tensor, freq, collapse_dim=False):\n",
    "    # Gets the component corresponding to cos(freq*(x+y)) in the 2D Fourier basis\n",
    "    # This is equivalent to the matrix cos((x+y)*freq*2pi/p)\n",
    "    cosx_cosy_direction = fourier_2d_basis_term(2*freq-1, 2*freq-1).flatten()\n",
    "    sinx_siny_direction = fourier_2d_basis_term(2*freq, 2*freq).flatten()\n",
    "    # Divide by sqrt(2) to ensure it remains normalised\n",
    "    cos_xpy_direction = (cosx_cosy_direction - sinx_siny_direction)/np.sqrt(2)\n",
    "    # Collapse_dim says whether to project back into R^(p*p) space or not\n",
    "    if collapse_dim:\n",
    "        return (cos_xpy_direction @ tensor)\n",
    "    else:\n",
    "        return cos_xpy_direction[:, None] @ (cos_xpy_direction[None, :] @ tensor)\n",
    "\n",
    "def get_component_sin_xpy(tensor, freq, collapse_dim=False):\n",
    "    # Gets the component corresponding to sin((x+y)*freq*2pi/p) in the 2D Fourier basis\n",
    "    sinx_cosy_direction = fourier_2d_basis_term(2*freq, 2*freq-1).flatten()\n",
    "    cosx_siny_direction = fourier_2d_basis_term(2*freq-1, 2*freq).flatten()\n",
    "    sin_xpy_direction = (sinx_cosy_direction + cosx_siny_direction)/np.sqrt(2)\n",
    "    if collapse_dim:\n",
    "        return (sin_xpy_direction @ tensor)\n",
    "    else:\n",
    "        return sin_xpy_direction[:, None] @ (sin_xpy_direction[None, :] @ tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "large_root = \"/home/hyeonbin/Arith_transfer/[Task: ADD] from scratch\"\n",
    "full_run_data = torch.load(f\"{large_root}/\" + \"full_run_data.pth\")\n",
    "\n",
    "num_layers = full_run_data['config']['num_layers']\n",
    "d_vocab = full_run_data['config']['d_vocab']\n",
    "d_model = full_run_data['config']['d_model']\n",
    "d_mlp = full_run_data['config']['d_mlp']\n",
    "d_head = full_run_data['config']['d_head']\n",
    "num_heads = full_run_data['config']['num_heads']\n",
    "n_ctx = full_run_data['config']['n_ctx']\n",
    "act_type = full_run_data['config']['act_type']\n",
    "use_ln = False\n",
    "\n",
    "model = Transformer(num_layers=num_layers, d_vocab=d_vocab, d_model=d_model, d_mlp=d_mlp, d_head=d_head, num_heads=num_heads, n_ctx=n_ctx, act_type=act_type, use_cache=False, use_ln=use_ln)\n",
    "model.to('cuda')\n",
    "model.load_state_dict(full_run_data['state_dicts'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_O = einops.rearrange(model.blocks[0].attn.W_O, 'm (i h)->i m h', i=num_heads)\n",
    "W_K = model.blocks[0].attn.W_K\n",
    "W_Q = model.blocks[0].attn.W_Q\n",
    "W_V = model.blocks[0].attn.W_V\n",
    "W_in = model.blocks[0].mlp.W_in\n",
    "W_out = model.blocks[0].mlp.W_out\n",
    "W_pos = model.pos_embed.W_pos.T\n",
    "# We remove the equals sign dimension from the Embed and Unembed, so we can\n",
    "# apply a Fourier Transform over R^p\n",
    "W_E = model.embed.W_E[:, :-1]\n",
    "W_U = model.unembed.W_U[:, :-1].T\n",
    "\n",
    "# The initial value of the residual stream at position 2 - constant for all inputs\n",
    "final_pos_resid_initial = model.embed.W_E[:, -1] + W_pos[:, 2]\n",
    "# print('W_O', W_O.shape)\n",
    "# print('W_K', W_K.shape)\n",
    "# print('W_Q', W_Q.shape)\n",
    "# print('W_V', W_V.shape)\n",
    "# print('W_in', W_in.shape)\n",
    "# print('W_out', W_out.shape)\n",
    "# print('W_pos', W_pos.shape)\n",
    "# print('W_E', W_E.shape)\n",
    "# print('W_U', W_U.shape)\n",
    "# print('Initial residual stream value at final pos:', final_pos_resid_initial.shape)\n",
    "\n",
    "p = 113\n",
    "\n",
    "if \"Task: ADD\" in large_root:\n",
    "    fn = lambda x, y: (x+y)%p\n",
    "elif \"Task: ADD_SQUARE\" in large_root:\n",
    "    fn = lambda x, y: (x + y)**2 % p\n",
    "elif \"Task: SQUARE_ADD\" in large_root:\n",
    "    fn = lambda x, y: (x**2 + y**2) % p\n",
    "\n",
    "all_data = torch.tensor([(i, j, p) for i in range(p) for j in range(p)]).to('cuda')\n",
    "labels = torch.tensor([fn(i, j) for i, j, _ in all_data]).to('cuda')\n",
    "cache = {}\n",
    "model.remove_all_hooks()\n",
    "model.cache_all(cache)\n",
    "# Final position only\n",
    "original_logits = model(all_data)[:, -1]\n",
    "# Remove equals sign from output logits\n",
    "original_logits = original_logits[:, :-1]\n",
    "original_loss = cross_entropy_high_precision(original_logits, labels)\n",
    "# print(f\"Original loss: {original_loss.item()}\")\n",
    "\n",
    "# Extracts out key activations\n",
    "attn_mat = cache['blocks.0.attn.hook_attn'][:, :, 2, :2]\n",
    "# print('Attention Matrix:', attn_mat.shape)\n",
    "neuron_acts = cache['blocks.0.mlp.hook_post'][:, -1]\n",
    "# print('Neuron Activations:', neuron_acts.shape)\n",
    "neuron_acts_pre = cache['blocks.0.mlp.hook_pre'][:, -1]\n",
    "# print('Neuron Activations Pre:', neuron_acts_pre.shape)\n",
    "\n",
    "p = 113\n",
    "\n",
    "fourier_basis = []\n",
    "fourier_basis.append(torch.ones(p)/np.sqrt(p))\n",
    "fourier_basis_names = ['Const']\n",
    "# Note that if p is even, we need to explicitly add a term for cos(kpi), ie\n",
    "# alternating +1 and -1\n",
    "for i in range(1, p//2 +1):\n",
    "    fourier_basis.append(torch.cos(2*torch.pi*torch.arange(p)*i/p))\n",
    "    fourier_basis.append(torch.sin(2*torch.pi*torch.arange(p)*i/p))\n",
    "    fourier_basis[-2]/=fourier_basis[-2].norm()\n",
    "    fourier_basis[-1]/=fourier_basis[-1].norm()\n",
    "    fourier_basis_names.append(f'cos {i}')\n",
    "    fourier_basis_names.append(f'sin {i}')\n",
    "fourier_basis = torch.stack(fourier_basis, dim=0).to('cuda')\n",
    "# animate_lines(fourier_basis, snapshot_index=fourier_basis_names, snapshot='Fourier Component', title='Graphs of Fourier Components (Use Slider)')\n",
    "\n",
    "# Center the neurons to remove the constant term\n",
    "neuron_acts_centered = neuron_acts - einops.reduce(neuron_acts, 'batch neuron -> 1 neuron', 'mean')\n",
    "# Note that fourier_neuron_acts[(0, 0), i]==0 for all i, because we centered the activations\n",
    "fourier_neuron_acts = fft2d(neuron_acts_centered)\n",
    "\n",
    "fourier_neuron_acts_square = fourier_neuron_acts.reshape(p, p, d_mlp)\n",
    "neuron_freqs = []\n",
    "neuron_frac_explained = []\n",
    "for ni in range(d_mlp):\n",
    "    best_frac_explained = 0\n",
    "    best_freq = -1\n",
    "    for freq in range(1, p//2):\n",
    "        # We extract the linear and quadratic fourier terms of frequency freq,\n",
    "        # and look at how much of the variance of the full vector this explains\n",
    "        # If neurons specialise into specific frequencies, one frequency should\n",
    "        # have a large value\n",
    "        frac_explained = (extract_freq_2d(fourier_neuron_acts_square[:, :, ni], freq).pow(2).sum()/\n",
    "                          fourier_neuron_acts_square[:, :, ni].pow(2).sum()).item()\n",
    "        if frac_explained > best_frac_explained:\n",
    "            best_freq = freq\n",
    "            best_frac_explained = frac_explained\n",
    "    neuron_freqs.append(best_freq)\n",
    "    neuron_frac_explained.append(best_frac_explained)\n",
    "    \n",
    "neuron_freqs = np.array(neuron_freqs)\n",
    "neuron_frac_explained = np.array(neuron_frac_explained)\n",
    "key_freqs, neuron_freq_counts = np.unique(neuron_freqs, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(x=neuron_freqs,\n",
    "        y=neuron_frac_explained,\n",
    "        labels={'x':'Neuron frequency',\n",
    "                'y':'Frac explained'},\n",
    "        color=to_numpy(einops.reduce((cache['blocks.0.mlp.hook_pre'][:, -1]>0).float(), 'batch neuron -> neuron', 'mean')),\n",
    "        color_continuous_scale='Viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_fourier(einops.reduce(fft2d(neuron_acts_centered).pow(2), 'batch neuron -> batch', 'sum'), \n",
    "               title='Norm of Fourier Components of Neuron Acts')\n",
    "imshow_fourier(einops.reduce(fft2d(original_logits).pow(2), 'batch vocab -> batch', 'sum'), \n",
    "               title='Norm of Fourier Components of Logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_logit = W_U @ W_out\n",
    "\n",
    "for freq in key_freqs:\n",
    "    imshow_div(fourier_basis @ W_logit[:, neuron_freqs==freq], aspect='auto', y=fourier_basis_names, xaxis='Neurons in cluster', title=f'W_logit in the Fourier Basis for neurons of freq {freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FW = fourier_basis @ W_logit\n",
    "\n",
    "arr = FW @ neuron_acts.T\n",
    "\n",
    "for freq in key_freqs[:1]:\n",
    "    imshow_fourier(fft2d(arr[2*freq-1]), title=f'cos {freq}')\n",
    "    imshow_fourier(fft2d(arr[2*freq]), title=f'sin {freq}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pre",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
